#pragma once
#include <grpcpp/server.h>

#include "absl/container/flat_hash_map.h"
#include "absl/container/flat_hash_set.h"
#include "boost/functional/hash.hpp"
#include "ray/common/asio/instrumented_io_context.h"
#include "ray/common/asio/periodical_runner.h"
#include "ray/common/id.h"
#include "src/ray/protobuf/ray_syncer.grpc.pb.h"

namespace ray {
namespace syncer {

using ray::rpc::syncer::DummyRequest;
using ray::rpc::syncer::DummyResponse;
using ray::rpc::syncer::RayComponentId;
using ray::rpc::syncer::RaySyncMessage;
using ray::rpc::syncer::RaySyncMessages;
using ray::rpc::syncer::RaySyncMessageType;
using ray::rpc::syncer::SyncMeta;

static constexpr size_t kComponentArraySize =
    static_cast<size_t>(ray::rpc::syncer::RayComponentId_ARRAYSIZE);

/// The interface for a reporter. Reporter is defined to be a local module which would
/// like to let the other nodes know its status. For example, local cluster resource
/// manager.
struct ReporterInterface {
  /// Interface to get the snapshot of the component. It asks the module to take a
  /// snapshot of the current status. Each snapshot is versioned, and it should return
  /// std::nullopt if the version hasn't changed.
  ///
  /// \param current_version The version syncer module current has.
  /// \param component_id The component id asked for.
  virtual std::optional<RaySyncMessage> Snapshot(uint64_t current_version,
                                                 RayComponentId component_id) const = 0;
  virtual ~ReporterInterface() {}
};

/// The interface for a receiver. Receiver is defined to be a module which would like
/// to get the status of other nodes. For example, cluster resource manager.
struct ReceiverInterface {
  /// Interface to update a module. The module should read the `sync_message` fields and
  /// deserialize it to update its internal status.
  ///
  /// \param message The message received from remote node.
  virtual void Update(std::shared_ptr<RaySyncMessage> message) = 0;
  /// Indicate whether the message received by this module needs to be
  /// broadcasted to other nodes.
  virtual bool NeedBroadcast() const = 0;
  virtual ~ReceiverInterface() {}
};

template <typename T>
using Array = std::array<T, kComponentArraySize>;

// Forward declaration of internal structures
class NodeStatus;
class NodeSyncConnection;

/// RaySyncer is an embedding service for component synchronization.
class RaySyncer {
 public:
  /// Constructor of RaySyncer
  ///
  /// \param node_id The id of current node.
  RaySyncer(const std::string &node_id);

  ~RaySyncer();

  void Connect(std::unique_ptr<NodeSyncConnection> context);

  void Disconnect(const std::string &node_id);

  /// Register the components to the syncer module. Syncer will make sure eventually
  /// it'll have a global view of the cluster.
  ///
  /// \param component_id The component to sync.
  /// \param reporter The local component to be broadcasted.
  /// \param receiver The snapshot of the component in the cluster.
  /// \param report_ms The frequence to report resource usages.
  void Register(RayComponentId component_id,
                const ReporterInterface *reporter,
                ReceiverInterface *receiver,
                int64_t report_ms = 100);

  /// Function to broadcast the messages to other nodes.
  /// A message will be sent to a node if that node doesn't have this message.
  /// The message can be generated by local reporter or received by the other node.
  ///
  /// \param message The message to be broadcasted.
  void BroadcastMessage(std::shared_ptr<RaySyncMessage> message);

  /// Get the current node id.
  const std::string &GetNodeId() const { return node_id_; }

  instrumented_io_context &GetIOContext() { return io_context_; }

  NodeSyncConnection *GetSyncConnection(const std::string &node_id) {
    auto iter = sync_context_.find(node_id);
    if (iter == sync_context_.end()) {
      return nullptr;
    }
    return iter->second.get();
  }

 private:
  /// The current node id.
  const std::string node_id_;

  /// Manage connections
  absl::flat_hash_map<std::string, std::unique_ptr<NodeSyncConnection>> sync_context_;

  /// The local node status
  std::unique_ptr<NodeStatus> node_status_;

  /// Threading for syncer. To have a better isolation, we put all operations in this
  /// module into a dedicated thread.
  std::unique_ptr<std::thread> syncer_thread_;
  instrumented_io_context io_context_;

  /// If the field is set to be true, it means, message generated or received
  /// is needed to be sent to other nodes.
  Array<bool> component_broadcast_ = {true};

  /// Timer is used to do broadcasting.
  ray::PeriodicalRunner timer_;
};

class ClientSyncConnection;
class ServerSyncConnection;

/// RaySyncerService is a service to take care of resource synchronization
/// related operations.
/// Right now only raylet needs to setup this service. But in the future,
/// we can use this to construct more complicated resource reporting algorithm,
/// like tree-based one.
class RaySyncerService : public ray::rpc::syncer::RaySyncer::CallbackService {
 public:
  RaySyncerService(RaySyncer &syncer) : syncer_(syncer) {}

  grpc::ServerUnaryReactor *StartSync(grpc::CallbackServerContext *context,
                                      const SyncMeta *request,
                                      SyncMeta *response) override {
    auto *reactor = context->DefaultReactor();
    // Make sure server only have one client
    RAY_CHECK(node_id_.empty());
    node_id_ = request->node_id();
    syncer_.Connect(std::make_unique<ServerSyncConnection>(
        syncer_, syncer_.GetIOContext(), request->node_id()));
    response->set_node_id(syncer_.GetNodeId());
    reactor->Finish(grpc::Status::OK);
    return reactor;
  }

  grpc::ServerUnaryReactor *Update(grpc::CallbackServerContext *context,
                                   const RaySyncMessages *request,
                                   DummyResponse *) override {
    auto *reactor = context->DefaultReactor();
    syncer_.GetIOContext.post(
        [this, request = std::move(*const_cast<RaySyncMessages *>(request))]() mutable {
          auto *sync_context =
              dynamic_cast<ServerSyncConnection *>(syncer_.GetSyncConnection(node_id_));
          if (sync_context != nullptr) {
            sync_context->ReceiveUpdate(std::move(request));
          } else {
            RAY_LOG(ERROR) << "Fail to get the sync context";
          }
        },
        "SyncerUpdate");
    reactor->Finish(grpc::Status::OK);
    return reactor;
  }

  grpc::ServerUnaryReactor *LongPolling(grpc::CallbackServerContext *context,
                                        const DummyRequest *,
                                        RaySyncMessages *response) override {
    auto *reactor = context->DefaultReactor();
    syncer_.GetIOContext.post(
        [this, reactor, response] mutable() {
          auto *sync_context =
              dynamic_cast<ServerSyncConnection *>(syncer_.GetSyncConnection(node_id_));
          if (sync_context != nullptr) {
            sync_context->HandleLongPollingRequest(reactor, response);
          } else {
            RAY_LOG(ERROR) << "Fail to setup long-polling";
            reactor->Finish(grpc::Status::OK);
          }
        },
        "SyncLongPolling");
    return reactor;
  }

 private:
  // This will be created after connection is established.
  // Ideally this should be owned by RaySyncer, but since we are doing
  // long-polling right now, we have to put it here so that when
  // long-polling request comes, we can set it up.
  std::string_id node_id_;

  // The ray syncer this RPC wrappers of.
  RaySyncer &syncer_;
};

}  // namespace syncer
}  // namespace ray

#include "ray/common/ray_syncer-inl.h"
